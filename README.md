# Tp3-Redes-Neurais Vision Transformer - OtimizaÃ§Ã£o no CIFAR-10

OtimizaÃ§Ã£o do Vision Transformer (ViT) no CIFAR-10 atravÃ©s de ajustes de batch size e data augmentation.

### ğŸ¯ Resultados
Melhor AcurÃ¡cia: 98.63% (batch 128 + RandAugment + CutMix)
Paper Original: 99.50% (com 20Ã— mais dados de prÃ©-treinamento)
DiferenÃ§a: Apenas -0.87%

### ğŸš€ ExecuÃ§Ã£o RÃ¡pida
Google Colab (Recomendado)

Abra Batch_ViT_treinamento.ipynb ou Augmentation_ViT_treinamento.ipynb
Configure GPU: Runtime â†’ Change runtime type â†’ GPU: A100 (Colab Pro recomendado)
Execute todas as cÃ©lulas

### ğŸ‘¨â€ğŸ’» Autor
Matheus Silva Palhares
CiÃªncia da ComputaÃ§Ã£o - UFV Campus Florestal
ğŸ“§ matheusspalhares@ufv.br

### ğŸ“š ReferÃªncias
Dosovitskiy et al. (2021) - "An Image is Worth 16x16 Words" (ICLR)
Cubuk et al. (2020) - "RandAugment" (CVPR)
Yun et al. (2019) - "CutMix" (ICCV)
